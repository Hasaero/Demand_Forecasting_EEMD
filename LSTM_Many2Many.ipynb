{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f0367e8",
   "metadata": {},
   "source": [
    "# LSTM Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353f79f",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "    - Raw data: Historical Product Demand.csv\n",
    "\n",
    "    - Input data: Data on 8x augmentation of demand records by selecting 8 representative items\n",
    "\n",
    "    - Product code: 'Product_0025', 'Product_0739', 'Product_0901', 'Product_1154',\n",
    "                    'Product_1248', 'Product_1295', 'Product_1378', 'Product_2004'\n",
    "            \n",
    "\n",
    "    - Size of Data: 116392 rows × 4 columns\n",
    "\n",
    "    - Features: Date, Product_Code, Product_Category, Order_Demand\n",
    "\n",
    "    - Period: 2012-01-01 ~ 2017-01-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9ee40",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c9a71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, date\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Save the log\n",
    "import os\n",
    "import time\n",
    "import pickle \n",
    "\n",
    "# LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import RepeatVector, TimeDistributed, Bidirectional\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Metric \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58e094",
   "metadata": {},
   "source": [
    "## Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c8f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "df = pd.read_csv('Data\\HPD_0416.csv')\n",
    "# convert the string to the datetype\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c008e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Product_Code</th>\n",
       "      <th>Product_Category</th>\n",
       "      <th>Order_Demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-10</td>\n",
       "      <td>Product_0025</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-11</td>\n",
       "      <td>Product_0025</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-12</td>\n",
       "      <td>Product_0025</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-13</td>\n",
       "      <td>Product_0025</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-14</td>\n",
       "      <td>Product_0025</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14459</th>\n",
       "      <td>2016-12-17</td>\n",
       "      <td>Product_2004</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14460</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>Product_2004</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14461</th>\n",
       "      <td>2016-12-19</td>\n",
       "      <td>Product_2004</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14462</th>\n",
       "      <td>2016-12-20</td>\n",
       "      <td>Product_2004</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>6000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14463</th>\n",
       "      <td>2016-12-21</td>\n",
       "      <td>Product_2004</td>\n",
       "      <td>Category_005</td>\n",
       "      <td>7000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14464 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Product_Code Product_Category  Order_Demand\n",
       "0     2012-01-10  Product_0025     Category_005         600.0\n",
       "1     2012-01-11  Product_0025     Category_005         800.0\n",
       "2     2012-01-12  Product_0025     Category_005         600.0\n",
       "3     2012-01-13  Product_0025     Category_005        1500.0\n",
       "4     2012-01-14  Product_0025     Category_005           0.0\n",
       "...          ...           ...              ...           ...\n",
       "14459 2016-12-17  Product_2004     Category_005           0.0\n",
       "14460 2016-12-18  Product_2004     Category_005           0.0\n",
       "14461 2016-12-19  Product_2004     Category_005           0.0\n",
       "14462 2016-12-20  Product_2004     Category_005        6000.0\n",
       "14463 2016-12-21  Product_2004     Category_005        7000.0\n",
       "\n",
       "[14464 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = pd.to_datetime('2012-01-10')\n",
    "end_date = pd.to_datetime('2016-12-21')\n",
    "\n",
    "df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f98bd020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('Data\\\\train.csv')\n",
    "# df = df[(df['store']==1)]\n",
    "\n",
    "# pd.to_datetime(df['date'].max()) - pd.to_datetime(df['date'].min())\n",
    "# df = df.reset_index(drop=True)\n",
    "# df.rename(columns={'date': 'Date', 'item':'Product_Code', 'sales':'Order_Demand'}, inplace=True)\n",
    "# df = df[['Date', 'Product_Code', 'Order_Demand']]\n",
    "# df['Date'] = pd.to_datetime(df['Date'])\n",
    "# df['Product_Code'] = df['Product_Code'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed38f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14464 entries, 0 to 14463\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   Date              14464 non-null  datetime64[ns]\n",
      " 1   Product_Code      14464 non-null  object        \n",
      " 2   Product_Category  14464 non-null  object        \n",
      " 3   Order_Demand      14464 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 452.1+ KB\n",
      "None\n",
      "-------------------------\n",
      "\n",
      "The Number of unique\n",
      "-------------------------\n",
      "Product code:\t 8\n",
      "Category:\t 5\n",
      "-------------------------\n",
      "The Product Code:\n",
      "\n",
      "1 Product_0025\n",
      "2 Product_0739\n",
      "3 Product_0901\n",
      "4 Product_1154\n",
      "5 Product_1248\n",
      "6 Product_1295\n",
      "7 Product_1378\n",
      "8 Product_2004\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print('-------------------------')\n",
    "print(\"\")\n",
    "print(\"The Number of unique\")\n",
    "print('-------------------------')\n",
    "print('Product code:\\t', df.Product_Code.nunique())\n",
    "print('Category:\\t', df.Product_Category.nunique())\n",
    "print('-------------------------')\n",
    "print(\"The Product Code:\")\n",
    "print(\"\")\n",
    "for i, code in enumerate(df['Product_Code'].unique()):\n",
    "    print(i+1, code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76850788",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680cb42",
   "metadata": {},
   "source": [
    "### Split the train and test set\n",
    "- Input\n",
    "     data: dataframe with dates and Demand data\n",
    "     \n",
    "- output\n",
    "    - train:  2012-01-10 ~ 2015-12-26    (4.5 Years)\n",
    "    - valid:  2015-12-26 ~ 2016-06-23    (0.5 Years)\n",
    "    - test :  2016-06-24 ~ 2016-12-21   (0.5 Years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e1b11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 IQR을 계산합니다.\n",
    "def replace_outlier(product_df, train_end):\n",
    "\n",
    "    train_df = product_df[:train_end]\n",
    "    valid_test_df = product_df[train_end:]\n",
    "\n",
    "    Q1 = train_df['y'].quantile(0.25)\n",
    "    Q3 = train_df['y'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outlier_condition = (train_df['y'] < (Q1 - 1.5 * IQR)) | (train_df['y'] > (Q3 + 1.5 * IQR))\n",
    "    mean_y = train_df.loc[~outlier_condition, 'y'].mean()\n",
    "\n",
    "    train_df.loc[outlier_condition, 'y'] = mean_y\n",
    "    noout_product_df = pd.concat([train_df, valid_test_df])\n",
    "    \n",
    "    return noout_product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "765de294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 IQR을 계산합니다.\n",
    "def replace_outlier(product_df, train_end):\n",
    "\n",
    "\n",
    "    Q1 = product_df['y'].quantile(0.25)\n",
    "    Q3 = product_df['y'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    outlier_condition = (product_df['y'] < (Q1 - 1.5 * IQR)) | (product_df['y'] > (Q3 + 1.5 * IQR))\n",
    "    mean_y = product_df.loc[~outlier_condition, 'y'].mean()\n",
    "\n",
    "    product_df.loc[outlier_condition, 'y'] = mean_y\n",
    "    \n",
    "    return product_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dbe9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(product_df, time_steps): \n",
    "\n",
    "#     train_end = len(product_df[product_df['Date']<'2015-12-26']) # train 데이터 개수\n",
    "#     val_end = len(product_df[product_df['Date']<'2016-06-23']) # validation 데이터 개수\n",
    "    val_end = len(product_df)-2*time_steps\n",
    "    train_end = val_end - 2*time_steps\n",
    "    y = product_df.filter(['y']).values # y(수요량) 값\n",
    "    \n",
    "    # Minmax로 0~1 사이에 값이 오도록 정규화\n",
    "    sc = MinMaxScaler() # 객체 생성\n",
    "    y_scaled = sc.fit_transform(y) # 전체 y값 정규화\n",
    "    # Train Data\n",
    "    #y_train_scaled = y_scaled[:train_end,:]\n",
    "    X_train = [] \n",
    "    y_train = []\n",
    "    for i in range(time_steps, train_end-time_steps): \n",
    "        X_train.append(y_scaled[i-time_steps:i,0]) \n",
    "        y_train.append(y_scaled[i:i+time_steps,0])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    # Reshape X_train for LSTM -> (batch_size, time_steps, features)\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "    y_train = np.reshape(y_train, (y_train.shape[0], y_train.shape[1],1))\n",
    "    \n",
    "    # Validation Data\n",
    "    X_val = []\n",
    "    y_val = []\n",
    "    \n",
    "    X_val.append(y_scaled[train_end:train_end+time_steps, 0])\n",
    "    y_val.append(y_scaled[train_end+time_steps:val_end, 0])\n",
    "\n",
    "    X_val = np.array(X_val)\n",
    "    y_val = np.array(y_val)\n",
    "    # Reshape X_val for LSTM -> (batch_size, time_steps, features)\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1],1))\n",
    "    y_val = np.reshape(y_val, (y_val.shape[0], y_val.shape[1],1))\n",
    "    \n",
    "    # Test Data\n",
    "    X_test = []\n",
    "    X_test.append(y_scaled[val_end:val_end+time_steps,0])\n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1],1))\n",
    "    \n",
    "    y_test = product_df.iloc[val_end+time_steps:,:]\n",
    "    y_test = y_test.copy()\n",
    "    y_test['y_norm'] = y_scaled[val_end+time_steps:].reshape(-1).copy()\n",
    "    # test data 개수만큼 반복\n",
    "#     for i in range(val_end+time_steps, len(y_scaled)-time_steps):\n",
    "#         X_test.append(y_scaled[i-time_steps : i, 0])\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da9661",
   "metadata": {},
   "source": [
    "## Optimized LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee2f32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import tempfile\n",
    "\n",
    "# 모델 구축 및 최적화 함수\n",
    "def build_model(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "    num_LSTM_layers = hp.Int('num_LSTM_layers', 1, 2)\n",
    "    for i in range(num_LSTM_layers):\n",
    "        if i == 0:\n",
    "            model.add(LSTM(units=hp.Int(f'LSTM_units_{i+1}', min_value=64, max_value=256, step=32),\n",
    "                           activation= 'tanh',\n",
    "                           input_shape=(None,1),\n",
    "                           return_sequences=True))\n",
    "        else:\n",
    "            model.add(LSTM(units=hp.Int(f'LSTM_units_{i+1}', min_value=64, max_value=256, step=32),\n",
    "                           activation= 'tanh',\n",
    "                           return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.2))\n",
    "    num_Dense_layers = hp.Int('num_Dense_layers', 1, 2)\n",
    "    for i in range(num_Dense_layers):\n",
    "        if i == 0:\n",
    "            model.add(TimeDistributed(Dense(hp.Int(f'Dense_units_{i+1}', min_value=16, max_value=64, step=16), \n",
    "                                        activation= 'relu')))\n",
    "        else:\n",
    "            model.add(TimeDistributed(Dense(hp.Int(f'Dense_units_{i+1}', min_value=16, max_value=32, step=16), \n",
    "                                        activation= 'relu')))\n",
    "        \n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', [1e-2, 1e-3, 5e-4])),\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics='mae')\n",
    "\n",
    "    return model\n",
    "\n",
    "# 모델 학습 및 평가 함수\n",
    "def optimize_model(X_train, y_train, X_val, y_val, X_test, sc, epochs, trials):\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        tuner = RandomSearch(\n",
    "            build_model,\n",
    "            objective='val_loss',\n",
    "            max_trials= trials,\n",
    "            directory=temp_dir,\n",
    "            project_name='temp_project')\n",
    "\n",
    "    tuner.search_space_summary()\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    tuner.search(X_train, y_train,\n",
    "                 epochs=epochs,\n",
    "                 batch_size=32,\n",
    "                 validation_data=(X_val, y_val),\n",
    "                 callbacks=[early_stopping])\n",
    "\n",
    "    tuner.results_summary()\n",
    "\n",
    "    best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    # Test 데이터 예측\n",
    "    pred = best_model.predict(X_test) # 예측값 얻기\n",
    "    pred_norm = pred # 예측값을 저장하되, normalize된 값 저장\n",
    "    pred = sc.inverse_transform(pred.reshape(pred.shape[0], pred.shape[1])) # denormalize된 예측값 저장\n",
    "    \n",
    "    best_model.summary()\n",
    "    # 모델 객체와 예측값 반환\n",
    "    return best_model, pred, pred_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "502f9b87",
   "metadata": {},
   "outputs": [],
   "source": [
    " def LSTM_single(product_df, time_steps, epochs, trials):\n",
    "\n",
    "    # 학습 데이터와 테스트 데이터 분리\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, sc = split_data(product_df, time_steps)\n",
    "    \n",
    "    # LSTM 모델 학습 및 예측\n",
    "    best_model, pred, pred_norm = optimize_model(X_train, y_train, X_val, y_val, X_test, sc, epochs, trials)\n",
    "    \n",
    "    # 예측 결과 저장\n",
    "    y_test.reset_index(drop=True, inplace=True)\n",
    "    pred_df = pd.DataFrame({'Pred': pred.reshape(-1) ,'Pred_norm': pred_norm.reshape(-1)})\n",
    "    res_df = pd.concat([y_test, pred_df], axis=1)\n",
    "    res_df.set_index('Date', inplace=True)\n",
    "    res_df.loc[res_df['Pred']<0, 'Pred']=0\n",
    "    \n",
    "    # res_df: ['y', 'y_norm', 'Pred', 'Pred_norm'], index='Date'\n",
    "    # 모델과 result_df\n",
    "    return best_model, res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd64a0",
   "metadata": {},
   "source": [
    "## Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d8fe2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_pred_plot(product_code, res_df, metric_df, normalize):\n",
    "    today = date.today()\n",
    "    \"\"\"\n",
    "    Plot the actual vs predition and save the figure in the given directory\n",
    "    \"\"\"\n",
    "    \n",
    "    save_path = os.path.join(\"Result\", \"Single_LSTM_Result\", product_code)\n",
    "    save_name = f'{product_code}_all_result'\n",
    "    \n",
    "    title = f\"Pred Actual Plot - {product_code}\"\n",
    "    actual = res_df['y']\n",
    "    pred = res_df['Pred']\n",
    "    \n",
    "    if normalize: \n",
    "        title += \"(Normalized)\"\n",
    "        actual = res_df['y_norm']\n",
    "        pred = res_df['Pred_norm']\n",
    "        save_name += \"_normalized\"\n",
    "    # Plot   \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.xlabel(\"Time\", fontsize=14)\n",
    "    plt.ylabel(\"Order Demand\", fontsize=14)\n",
    "    plt.plot(actual, label ='Actual', alpha=0.6, marker='o', ms=3)\n",
    "    plt.plot(pred, label='Prediction', alpha=0.8, marker='o', ms=3)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "        \n",
    "    # Plot 결과 저장\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    # save the figure\n",
    "    today_date = f'_{today.month:02d}{today.day:02d}'\n",
    "    plt.savefig(os.path.join(save_path, save_name+'.png'))\n",
    "    # Metric도 함께 저장\n",
    "    metric_df.to_csv(os.path.join(save_path, save_name+'.csv'))\n",
    "        \n",
    "    plt.close('all') # close all figures to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c10393",
   "metadata": {},
   "source": [
    "## Save and Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db0243bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(product_code, best_model):\n",
    "    today = date.today()\n",
    "    folder_path = 'Result/Single_LSTM_Result/Model'\n",
    "    file_name = f'{product_code}_{today.month:02d}{today.day:02d}.pkl'\n",
    "    save_path = os.path.join(folder_path, file_name)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "    # 객체를 pickle 파일로 저장\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e960cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_name):\n",
    "    file_path = f'Result/Single_LSTM_Result/Model/{file_name}'\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        model_dict= pickle.load(file)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd8454",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28dd74b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Metric\n",
    "def mase(training_series, testing_series, prediction_series):\n",
    "    n = training_series.shape[0]\n",
    "    d = np.abs(np.diff(training_series)).sum() / (n-1)\n",
    "    \n",
    "    errors = np.abs(testing_series - prediction_series)\n",
    "    return errors.mean() / d\n",
    "\n",
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.mean(np.abs((actual - pred) / (actual+1)))\n",
    "\n",
    "# 정규화 된 지표\n",
    "def nrmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    target_mean = np.mean(y_true)\n",
    "    nrmse = mse / target_mean\n",
    "    return nrmse\n",
    "\n",
    "# 정규화 된 지표\n",
    "def nmae(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    target_mean = np.mean(y_true)\n",
    "    nmae = mae / target_mean\n",
    "    return nmae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83451baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(product_code, res_df, normalize):\n",
    "    # 정규화 옵션이 True인 경우 정규화된 데이터 사용, 그렇지 않으면 원래 데이터 사용\n",
    "    if normalize:\n",
    "        actual = res_df['y_norm']\n",
    "        pred = res_df['Pred_norm']\n",
    "    else:\n",
    "        actual = res_df['y']\n",
    "        pred = res_df['Pred']\n",
    "\n",
    "    # 메트릭 계산\n",
    "    # MASE = mase(np.array(train_series), np.array(actual), pred) \n",
    "    MAPE = mape(actual, pred) \n",
    "    RMSE = mean_squared_error(actual, pred)**0.5 \n",
    "    MAE = mean_absolute_error(actual,pred) \n",
    "    NRMSE = nrmse(actual,pred) \n",
    "    NMAE = nmae(actual,pred)\n",
    "    R2 = r2_score(actual,pred) \n",
    "    # RMSLE = mean_squared_log_error(actual, pred)**0.5 \n",
    "\n",
    "    # 계산된 메트릭을 데이터프레임에 추가\n",
    "    metric_df = pd.DataFrame({'MAPE':[round(MAPE, 4)],\n",
    "                           'RMSE':[round(RMSE, 4)],\n",
    "                           'MAE':[round(MAE, 4)],\n",
    "                           'NRMSE':[round(NRMSE, 4)],\n",
    "                           'NMAE':[round(NMAE, 4)],\n",
    "                           'R2':[round(R2, 4)]},\n",
    "                            index= [product_code])\n",
    "\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073bc376",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e45cb6",
   "metadata": {},
   "source": [
    "## Check the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2bffe",
   "metadata": {},
   "source": [
    "## Whole Process\n",
    "    - product_code에 str으로 예측하고자 하는 코드를 입력\n",
    "    - ['Product_0025', 'Product_0739', 'Product_0901', 'Product_1154',\n",
    "       'Product_1248', 'Product_1295', 'Product_1378', 'Product_2004']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ce9ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_single_LSTM(product_code, time_steps=90, epochs=100, optimize_trials=10):\n",
    "    start_time = time.time()\n",
    "    product_code = product_code # 예측하고자 하는 코드 입력\n",
    "    product_df = df[df['Product_Code']== product_code].reset_index(drop=True)\n",
    "    product_df = product_df[['Date', 'Order_Demand']]\n",
    "    product_df.rename(columns={'Order_Demand': 'y'}, inplace=True)\n",
    "\n",
    "    # LSTM 단일 모델\n",
    "    model, res_df = LSTM_single(product_df, time_steps, epochs, optimize_trials) #dictionary, time_steps, epochs\n",
    "    save_model(product_code, model)\n",
    "    # 모델 Metric과 Pred_Actual Plot 저장\n",
    "    metric_df_norm = calculate_metrics(product_code, res_df, True)\n",
    "    metric_df= calculate_metrics(product_code, res_df, False)\n",
    "    \n",
    "    actual_pred_plot(product_code, res_df, metric_df_norm, True)\n",
    "    actual_pred_plot(product_code, res_df, metric_df, False)\n",
    "    # 실행시간 확인\n",
    "    elapsed_time_seconds = time.time() - start_time\n",
    "    elapsed_time_minutes = elapsed_time_seconds / 60\n",
    "    print(\"실행 시간: {:.2f} 분\".format(elapsed_time_minutes))\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5269ad",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "095eda0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 08s]\n",
      "val_loss: 0.012715483084321022\n",
      "\n",
      "Best val_loss So Far: 0.011502848006784916\n",
      "Total elapsed time: 00h 03m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "Results summary\n",
      "Results in C:\\Users\\7info\\AppData\\Local\\Temp\\tmpp0s0qhd3\\temp_project\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 256\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 48\n",
      "learning_rate: 0.01\n",
      "LSTM_units_2: 64\n",
      "Dense_units_2: 16\n",
      "Score: 0.011502848006784916\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 128\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 16\n",
      "learning_rate: 0.0005\n",
      "LSTM_units_2: 64\n",
      "Dense_units_2: 32\n",
      "Score: 0.012617578729987144\n",
      "\n",
      "Trial 00 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 2\n",
      "LSTM_units_1: 128\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 16\n",
      "learning_rate: 0.0005\n",
      "LSTM_units_2: 64\n",
      "Score: 0.012673303484916687\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 256\n",
      "num_Dense_layers: 2\n",
      "Dense_units_1: 32\n",
      "learning_rate: 0.001\n",
      "LSTM_units_2: 128\n",
      "Dense_units_2: 32\n",
      "Score: 0.012697306461632252\n",
      "\n",
      "Trial 01 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 64\n",
      "num_Dense_layers: 2\n",
      "Dense_units_1: 32\n",
      "learning_rate: 0.01\n",
      "LSTM_units_2: 192\n",
      "Dense_units_2: 16\n",
      "Score: 0.012705964036285877\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 2\n",
      "LSTM_units_1: 192\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 32\n",
      "learning_rate: 0.01\n",
      "LSTM_units_2: 256\n",
      "Dense_units_2: 16\n",
      "Score: 0.012715274468064308\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 128\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 32\n",
      "learning_rate: 0.001\n",
      "LSTM_units_2: 160\n",
      "Dense_units_2: 32\n",
      "Score: 0.012715483084321022\n",
      "\n",
      "Trial 02 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 224\n",
      "num_Dense_layers: 1\n",
      "Dense_units_1: 48\n",
      "learning_rate: 0.0005\n",
      "LSTM_units_2: 64\n",
      "Dense_units_2: 32\n",
      "Score: 0.012737317010760307\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 2\n",
      "LSTM_units_1: 224\n",
      "num_Dense_layers: 2\n",
      "Dense_units_1: 32\n",
      "learning_rate: 0.001\n",
      "LSTM_units_2: 224\n",
      "Dense_units_2: 32\n",
      "Score: 0.012815257534384727\n",
      "\n",
      "Trial 04 summary\n",
      "Hyperparameters:\n",
      "num_LSTM_layers: 1\n",
      "LSTM_units_1: 128\n",
      "num_Dense_layers: 2\n",
      "Dense_units_1: 48\n",
      "learning_rate: 0.01\n",
      "LSTM_units_2: 64\n",
      "Dense_units_2: 16\n",
      "Score: 0.012981814332306385\n",
      "1/1 [==============================] - 0s 357ms/step\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, None, 256)         264192    \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 48)         12336     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, None, 1)          49        \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,577\n",
      "Trainable params: 276,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "실행 시간: 3.05 분\n"
     ]
    }
   ],
   "source": [
    "for code in ['Product_0025', 'Product_0739', 'Product_0901', 'Product_1154',\n",
    "             'Product_1248', 'Product_1295', 'Product_1378', 'Product_2004']:\n",
    "    print(\"==================================\")\n",
    "    print(f\"========== { code } ==========\")\n",
    "    print(\"==================================\")\n",
    "    execute_single_LSTM(code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EEMD_LSTM",
   "language": "python",
   "name": "eemd_lstm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
